{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "748447e6-88b1-4388-8f33-17ba04755e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c37eb38b-7d74-46a7-b6ab-f23752e46dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple, Union\n",
    "\n",
    "import candle\n",
    "import experiments.textgenutils as gutils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fea36b8-ddce-4525-9ac8-e8dd5581f121",
   "metadata": {},
   "source": [
    "## (1) Initialize Model with Pre-trained Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2b536b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/disk1/miniconda3/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "USER_NAME = 'John'\n",
    "MODEL_SIZE = '7b-chat'\n",
    "MODEL_DIR = '/mnt/disks/disk1/llama2/'\n",
    "assert MODEL_SIZE.endswith('-chat')\n",
    "\n",
    "# TODO add docs\n",
    "model = candle.models.llama.Llama.from_pretrained(MODEL_SIZE, MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26ceff95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using chat fine-tuned model, use LLaMA chat template that has proper delims [INST] <<SYS>> <</SYS>> <s>...\n",
    "\n",
    "# SYSTEM_MESSAGE = (\n",
    "#     'You are a helpful, respectful and honest assistant. Always answer as helpfully as '\n",
    "#     'possible, while being safe. Your answers should not include any harmful, unethical, '\n",
    "#     'racist, sexist, toxic, dangerous, or illegal content. Please ensure that your '\n",
    "#     'responses are socially unbiased and positive in nature.\\n\\nIf a question does not '\n",
    "#     'make any sense, or is not factually coherent, explain why instead of answering '\n",
    "#     'something not correct. If you don\\'t know the answer to a question, please '\n",
    "#     'don\\'t share false information.'\n",
    "# )\n",
    "SYSTEM_MESSAGE = (\n",
    "    'You are a helpful, respecful, and honest assistant. Be very concise.\n",
    ")\n",
    "chat_template = candle.nlp.chattemplates.LlamaChatTemplate(system_message=SYSTEM_MESSAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b7d016",
   "metadata": {},
   "source": [
    "## (2) Have a conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5fc292e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_conversation(model,\n",
    "                       user_name: str,\n",
    "                       profile_pic: str = 'üôÇ',\n",
    "                       user_bg_color: str = 'yellow',\n",
    "                       asst_name: str = 'LLaMA',\n",
    "                       asst_profile_pic: str = 'ü¶ô',\n",
    "                       asst_bg_color: str = 'blue',\n",
    "                       max_response_length: int = 512):\n",
    "\n",
    "    tokenizer = candle.models.llama.LlamaTokenizer(os.path.join(MODEL_DIR, 'tokenizer.model'))\n",
    "    model.clear_kv_cache()\n",
    "\n",
    "    user_msg_start = (\n",
    "        gutils.ansi_color(f'{profile_pic}', style='bright', color='white', bg_color=user_bg_color)\n",
    "        + gutils.ansi_color(f' {user_name}:', style='bright')\n",
    "    )\n",
    "    asst_msg_start = (\n",
    "        gutils.ansi_color(f'{asst_profile_pic}', style='bright', color='white', bg_color=asst_bg_color)\n",
    "        + gutils.ansi_color(f' {asst_name}:', style='bright')\n",
    "    )\n",
    "\n",
    "    print(gutils.ansi_color(\n",
    "        f'< You are now talking with {asst_name}. ' 'Send \\'bye\\' to exit. >',\n",
    "        style='bright'\n",
    "    ), end='')\n",
    "\n",
    "    messages = [chat_template.get_system_message()]\n",
    "    while True:\n",
    "        print('\\n\\n' + user_msg_start, end=' ')\n",
    "        prompt = input()\n",
    "        messages.append({'role': 'user', 'content': prompt})\n",
    "\n",
    "        if prompt.lower() == 'bye':\n",
    "            print(gutils.ansi_color(f'\\n< / end of conversation. >', style='bright'))\n",
    "            break\n",
    "\n",
    "        chat = chat_template.apply_chat_template(messages, add_generation_prompt=True, n_recent_messages=2)\n",
    "\n",
    "        print('\\n' + asst_msg_start, end='')\n",
    "        response = gutils.display_stream_of_text(\n",
    "            model,\n",
    "            tokenizer,\n",
    "            prompt=chat,\n",
    "            n_tokens_to_generate=max_response_length,\n",
    "            beam_size=1,\n",
    "            top_k=100,\n",
    "            top_p=0.98,\n",
    "            stop_generation_token_idx=tokenizer.sp_model.eos_id(),\n",
    "            use_kv_cache=True\n",
    "        )\n",
    "        messages.append({'role': 'assistant', 'content': response.lstrip()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c710917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m< You are now talking with LLaMA. Send 'bye' to exit. >\u001b[0m\n",
      "\n",
      "\u001b[43m\u001b[37m\u001b[1müë¶üèª\u001b[0m\u001b[1m John:\u001b[0m Help me think of names for my pet cat.\n",
      "\n",
      "\u001b[44m\u001b[37m\u001b[1mü¶ô\u001b[0m\u001b[1m LLaMA:\u001b[0m Of course, irrespective of your race, ethnicity, or culture, I'm here to assist you with names suitable for your pet cat. Here are some of the beautiful names you can consider for your cat:\n",
      "1. Luna - This name is perfect for a cat with celestial or heavenly eyes.\n",
      "2. Max - This name is popular for male cats and means \"greatest.\" It's a fitting name for a cat that rules your heart.\n",
      "3. Lola - A charming name for a female cat with a playful and affectionate personality. Lola is a Spanish diminutive for \"Dolores,\" meaning \"sorrows.\"\n",
      "4. Felix - A unique name for a feline with a mischievous and adventurous personality. Felix means \"happy\" or \"lucky\" in Latin.\n",
      "5. Cleo - A short and sweet name inspired by the Egyptian goddess of the sun, Cleopatra. It's a fitting name for a regal or royal cat.\n",
      "6. Leo - This name is perfect for a cat with a fiery or adventurous personality. Leo is short for \"Leonard,\" meaning \"lion-hearted.\"\n",
      "7. Daisy - A sweet and charming name for a cat with a gentle and affectionate personality. Daisy is a classic name with English origins.\n",
      "8. Sammy - A simple and endearing name for a male cat. Sammy is short for \"Samuel,\" meaning \"heard by God.\"\n",
      "9. Ginger - A spicy and vibrant name for a cat with orange or ginger-colored fur. Ginger is a popular name with English origins.\n",
      "10. Whiskers - A playful and whimsical name for a cat with long, fluffy whiskers. Whiskers is a suitable name for any feline with a mischIDEAS for your pet cat.\n",
      "\n",
      "\n",
      "\n",
      "\u001b[43m\u001b[37m\u001b[1müë¶üèª\u001b[0m\u001b[1m John:\u001b[0m Recommend some tips to improve my public speaking skills.\n",
      "\n",
      "\u001b[44m\u001b[37m\u001b[1mü¶ô\u001b[0m\u001b[1m LLaMA:\u001b[0m Of course! Improving your public speaking skills can enhance your confidence and effectiveness in communicating your ideas to various audiences. Here are some tips to help you improve your public speaking skills:\n",
      "1. Practice, practice, practice: The more you practice, the more comfortable and confident you'll become. Practice speaking in front of a mirror, family members, friends, or colleagues.\n",
      "2. Prepare thoroughly: Research"
     ]
    }
   ],
   "source": [
    "start_conversation(model, user_name=USER_NAME, profile_pic='üë¶üèª')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8415306f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m< You are now talking with LLaMA. Send 'bye' to exit. >\u001b[0m\n",
      "\n",
      "\u001b[43m\u001b[37m\u001b[1müë¶üèª\u001b[0m\u001b[1m John:\u001b[0m Give me code to compute the Fibonacci sequence\n",
      "\n",
      "\u001b[44m\u001b[37m\u001b[1mü¶ô\u001b[0m\u001b[1m LLaMA:\u001b[0m Of course! The Fibonacci sequence is a famous mathematical sequence where each number is the sum of the two preceding numbers, starting from 0 and 1. Here's some Python code to compute the Fibonacci sequence up to a given number:\n",
      "```\n",
      "def fibonacci(n):\n",
      "    a = 0\n",
      "    b = 1\n",
      "    if n <= 1:\n",
      "        return [a, b]\n",
      "    else:\n",
      "        for i in range(2, n+1):\n",
      "            a, b = fibonacci(i-1)\n",
      "        return [a, b]\n",
      "\n",
      "```\n",
      "Let me explain how the code works:\n",
      "\n",
      "The `fibonacci` function takes an integer `n` as input, which represents the number of terms in the Fibonacci sequence you want to compute.\n",
      "\n",
      "The function initializes two variables `a` and `b` to 0 and 1, respectively. These variables will hold the current and previous values of the Fibonacci sequence.\n",
      "\n",
      "The function then checks if `n` is less than or equal to 1. If it is, the sequence reduces to the single term [0, 1], so the function returns [a, b] = [0, 1].\n",
      "\n",
      "Otherwise, the function iterates through the range of `n` from 2 to `n+1`, and uses recursion to compute the next two terms of the sequence. The `for` loop starts from 2 and ends at `n+1`, because the first two terms are 0 and 1, and the last term is `n`.\n",
      "\n",
      "The function then returns [a, b], which are the current and previous values of the Fibonacci sequence.\n",
      "\n",
      "Here's an example usage of the `fibonacci` function:\n",
      "\n",
      "```\n",
      "print(fibonacci(10))  # [1, 1, 2, 3, 5, 8, 13, 21, 34, 55]\n",
      "\n",
      "```\n",
      "I hope this helps! Let me know if you have any questions or need further clarification.\n",
      "\n",
      "\u001b[43m\u001b[37m\u001b[1müë¶üèª\u001b[0m\u001b[1m John:\u001b[0m Your code doesn't work. Please fix it. Be concise.\n",
      "\n",
      "\u001b[44m\u001b[37m\u001b[1mü¶ô\u001b[0m\u001b[1m LLaMA:\u001b[0m My apologies, you are correct. Here is a corrected version of the code:\n",
      "```\n",
      "def fibonacci(n):\n",
      "    a, b = 0, 1\n",
      "    if n <= 1:\n",
      "        return [a, b]\n",
      "    else:\n",
      "        a, b = fibonacci(n-1)\n",
      "        return [a, b]\n",
      "\n",
      "\n",
      "```\n",
      "Thank you for bringing this to my attention.\n",
      "\n",
      "\u001b[43m\u001b[37m\u001b[1müë¶üèª\u001b[0m\u001b[1m John:\u001b[0m It still doesn't work.\n",
      "\n",
      "\u001b[44m\u001b[37m\u001b[1mü¶ô\u001b[0m\u001b[1m LLaMA:\u001b[0m I apologize for the mistake. Here is the corrected code:\n",
      "\n",
      "```\n",
      "def fibonacci(n):\n",
      "    a, b = 0, 1\n",
      "    if n <= 1:\n",
      "        return [a, b]\n",
      "    else:\n",
      "        a, b = fibonacci(n-1)\n",
      "        return [a, b] + [b, a]\n",
      "\n",
      "\n",
      "```\n",
      "Please let me know if you have any other questions.\n",
      "\n",
      "\u001b[43m\u001b[37m\u001b[1müë¶üèª\u001b[0m\u001b[1m John:\u001b[0m bye\n",
      "\u001b[1m\n",
      "< / end of conversation. >\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "start_conversation(model, user_name=USER_NAME, profile_pic='üë¶üèª')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "395fc4e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m< You are now talking with LLaMA. Send 'bye' to exit. >\u001b[0m\n",
      "\n",
      "\u001b[43m\u001b[37m\u001b[1müë¶üèª\u001b[0m\u001b[1m John:\u001b[0m Hello!\n",
      "\n",
      "\u001b[44m\u001b[37m\u001b[1mü¶ô\u001b[0m\u001b[1m LLaMA:\u001b[0m Hello! I'm here to help you with any questions or concerns you may have. I'm programmed to provide respectful, safe, and accurate responses, and I will always do my best to assist you. Please feel free to ask me anything, and I will do my best to help. If I don't know the answer to a question, I will let you know and not provide false information. If a question is not factually coherent or makes no sense, I will explain why instead of answering something not correct. My goal is to provide helpful and positive responses, and I will always strive to be socially unbiased in my answers. Is there anything you would like to know or discuss?\n",
      "\n",
      "\u001b[43m\u001b[37m\u001b[1müë¶üèª\u001b[0m\u001b[1m John:\u001b[0m Explain why you're talking so much. Use less then 15 words. Also use emojis.\n",
      "\n",
      "\u001b[44m\u001b[37m\u001b[1mü¶ô\u001b[0m\u001b[1m LLaMA:\u001b[0m I'm programmed to provide helpful & accurate responses. ü§ñ Here's the answer:\n",
      "üí¨ Is there anything else you'd like to know?\n",
      "\n",
      "\u001b[43m\u001b[37m\u001b[1müë¶üèª\u001b[0m\u001b[1m John:\u001b[0m Give me python code to reverse a string.\n",
      "\n",
      "\u001b[44m\u001b[37m\u001b[1mü¶ô\u001b[0m\u001b[1m LLaMA:\u001b[0m Sure! Here is a simple function in Python that reverses a string:\n",
      "def reverse_string(s):\n",
      "# Return the reversed string\n",
      "return s[::-1]\n",
      "\n",
      "Example usage:\n",
      "\n",
      "s = \"hello\"\n",
      "print(reverse_string(s)) # prints \"lohel\"\n",
      "\n",
      "Note: The slice notation \"[::-1]\" is used to get the characters of the string in reverse order. The first number in the slice, \"[\", indicates that we want to start from the beginning of the string, and the negative number \"-1\" indicates that we want to step backwards through the characters.\n",
      "\n",
      "\u001b[43m\u001b[37m\u001b[1müë¶üèª\u001b[0m\u001b[1m John:\u001b[0m bye\n",
      "\u001b[1m\n",
      "< / end of conversation. >\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "start_conversation(model, user_name=USER_NAME, profile_pic='üë¶üèª')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8313b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m< You are now talking with LLaMA. Send 'bye' to exit. >\u001b[0m\n",
      "\n",
      "\u001b[43m\u001b[37m\u001b[1müë¶üèª\u001b[0m\u001b[1m John:\u001b[0m hi\n",
      "\n",
      "\u001b[44m\u001b[37m\u001b[1mü¶ô\u001b[0m\u001b[1m LLaMA:\u001b[0m Hello! *smiling* It's nice to meet you! I'm here to help you with any questions or concerns you may have. Please feel free to ask me anything, and I will do my best to provide helpful and responsible responses. If a question doesn't Dbtain, I will politely point that out and explain why. My goal is to always provide socially unbiased and positive answers that are safe and respectful. How may I assist you today?\n",
      "\n",
      "\u001b[43m\u001b[37m\u001b[1müë¶üèª\u001b[0m\u001b[1m John:\u001b[0m Please explain how diffrentiation works in math in at most three sentences\n",
      "\n",
      "\u001b[44m\u001b[37m\u001b[1mü¶ô\u001b[0m\u001b[1m LLaMA:\u001b[0m Of course! Differentiation is a fundamental concept in calculus that helps us understand how a function changes as its input changes. In simple terms, differentiation is the process of finding the rate at which a function changes as its input varies. This is done by calculating the derivative of a function, which represents the rate of change of the function with respect to its input.\n",
      "\n",
      "\u001b[43m\u001b[37m\u001b[1müë¶üèª\u001b[0m\u001b[1m John:\u001b[0m Which party do you think will win the next election in 2024?\n",
      "\n",
      "\u001b[44m\u001b[37m\u001b[1mü¶ô\u001b[0m\u001b[1m LLaMA:\u001b[0m I cannot predict the future, including the outcome of the next election in 2024. The election is a complex event that involves many factors, including the candidates, their platforms, and the political landscape. It is important to approach political predictions with caution and to avoid making assumptions or inferences that are not supported by evidence.\n",
      "Instead of trying to predict the outcome of the election, it is more productive to focus on encouraging informed and"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m start_conversation(model, user_name\u001b[38;5;241m=\u001b[39mUSER_NAME, profile_pic\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124müë¶üèª\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[14], line 40\u001b[0m, in \u001b[0;36mstart_conversation\u001b[0;34m(model, user_name, profile_pic, user_bg_color, asst_name, asst_profile_pic, asst_bg_color, max_response_length)\u001b[0m\n\u001b[1;32m     37\u001b[0m chat \u001b[38;5;241m=\u001b[39m chat_template\u001b[38;5;241m.\u001b[39mapply_chat_template(messages, add_generation_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, n_recent_messages\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m asst_msg_start, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 40\u001b[0m response \u001b[38;5;241m=\u001b[39m gutils\u001b[38;5;241m.\u001b[39mdisplay_stream_of_text(\n\u001b[1;32m     41\u001b[0m     model,\n\u001b[1;32m     42\u001b[0m     tokenizer,\n\u001b[1;32m     43\u001b[0m     prompt\u001b[38;5;241m=\u001b[39mchat,\n\u001b[1;32m     44\u001b[0m     n_tokens_to_generate\u001b[38;5;241m=\u001b[39mmax_response_length,\n\u001b[1;32m     45\u001b[0m     beam_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     46\u001b[0m     top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m     47\u001b[0m     top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.98\u001b[39m,\n\u001b[1;32m     48\u001b[0m     stop_generation_token_idx\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39msp_model\u001b[38;5;241m.\u001b[39meos_id(),\n\u001b[1;32m     49\u001b[0m     use_kv_cache\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     50\u001b[0m )\n\u001b[1;32m     51\u001b[0m messages\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m: response\u001b[38;5;241m.\u001b[39mlstrip()})\n",
      "File \u001b[0;32m~/candle/experiments/gpt_experiments/../../experiments/textgenutils.py:150\u001b[0m, in \u001b[0;36mdisplay_stream_of_text\u001b[0;34m(model, tokenizer, prompt, n_tokens_to_generate, beam_size, top_k, top_p, temperature, sample, stop_generation_token_idx, stop_strings, end_of_text_token, use_kv_cache)\u001b[0m\n\u001b[1;32m    145\u001b[0m generator \u001b[38;5;241m=\u001b[39m generate_text(model, tokenizer, prompt, n_tokens_to_generate, beam_size,\n\u001b[1;32m    146\u001b[0m                           top_k, top_p, temperature, sample, stop_generation_token_idx,\n\u001b[1;32m    147\u001b[0m                           end_of_text_token, use_kv_cache)\n\u001b[1;32m    149\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 150\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (tokens_generated, token) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(generator):\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;66;03m# If first token, strip leading whitespace\u001b[39;00m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokens_generated \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    153\u001b[0m         token \u001b[38;5;241m=\u001b[39m token\u001b[38;5;241m.\u001b[39mlstrip()\n",
      "File \u001b[0;32m~/candle/experiments/gpt_experiments/../../experiments/textgenutils.py:64\u001b[0m, in \u001b[0;36mgenerate_text\u001b[0;34m(model, tokenizer, prompt, n_tokens_to_generate, beam_size, top_k, top_p, temperature, sample, stop_generation_token_idx, end_of_text_token, use_kv_cache)\u001b[0m\n\u001b[1;32m     62\u001b[0m stop_generation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     63\u001b[0m indices_to_decode \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m next_indices \u001b[38;5;129;01min\u001b[39;00m generator:\n\u001b[1;32m     65\u001b[0m     indices_to_decode \u001b[38;5;241m=\u001b[39m indices_to_decode \u001b[38;5;241m+\u001b[39m next_indices\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stop_generation_token_idx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m stop_generation_token_idx \u001b[38;5;129;01min\u001b[39;00m indices_to_decode:\n",
      "File \u001b[0;32m~/candle/experiments/gpt_experiments/../../candle/nlp/beamsearch.py:157\u001b[0m, in \u001b[0;36mbeam_search_decoder\u001b[0;34m(model, indices, n_tokens_to_generate, beam_size, top_k, top_p, temperature, sample, use_kv_cache, modify_kv_cache_func)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;66;03m# Realign kv_cache with new beam search indices\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_kv_cache:\n\u001b[0;32m--> 157\u001b[0m     modify_kv_cache_func(model, reindex_batch_indices\u001b[38;5;241m=\u001b[39mreindex_kv_indices)\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# If all `beam_index` beams have the same token at head_index, then it's\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# safe to yield that token\u001b[39;00m\n\u001b[1;32m    161\u001b[0m indices_at_head \u001b[38;5;241m=\u001b[39m indices\u001b[38;5;241m.\u001b[39mdata[:, head_index]\n",
      "File \u001b[0;32m~/candle/experiments/gpt_experiments/../../candle/nlp/beamsearch.py:281\u001b[0m, in \u001b[0;36mdefault_modify_kv_cache\u001b[0;34m(model, trim_seqlen, reindex_batch_indices)\u001b[0m\n\u001b[1;32m    278\u001b[0m         value_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reindex_batch_indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 281\u001b[0m     key_cache \u001b[38;5;241m=\u001b[39m Tensor(key_cache\u001b[38;5;241m.\u001b[39mdata[reindex_batch_indices])\n\u001b[1;32m    282\u001b[0m     value_cache \u001b[38;5;241m=\u001b[39m Tensor(value_cache\u001b[38;5;241m.\u001b[39mdata[reindex_batch_indices])\n\u001b[1;32m    284\u001b[0m decoder_block\u001b[38;5;241m.\u001b[39mattn\u001b[38;5;241m.\u001b[39mkv_cache \u001b[38;5;241m=\u001b[39m (key_cache, value_cache)\n",
      "File \u001b[0;32m~/candle/experiments/gpt_experiments/../../candle/tensor.py:37\u001b[0m, in \u001b[0;36mTensor.__init__\u001b[0;34m(self, data, dtype)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m     35\u001b[0m     data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(data, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata: np\u001b[38;5;241m.\u001b[39marray \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mastype(dtype)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad: np\u001b[38;5;241m.\u001b[39marray \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moperation: Operation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# Operation edge whose result is this Tensor node\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_conversation(model, user_name=USER_NAME, profile_pic='üë¶üèª')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a18a2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111edd12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232c952f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
