{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f7ff890",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f70c3a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import candle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170016c9",
   "metadata": {},
   "source": [
    "## (1) Initialize Target and Draft Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d13c68ec-e961-4526-a686-9bb8e3281b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file from cache: /home/johnma2006/.cache/candle/gpt2_encoder.json\n",
      "Loading file from cache: /home/johnma2006/.cache/candle/gpt2_vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "target_model = candle.models.gpt.GPT.from_pretrained('gpt2-large')\n",
    "draft_model = candle.models.gpt.GPT.from_pretrained('gpt2')\n",
    "tokenizer = candle.models.gpt.GPT2BPETokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146a46ad-6cb6-42ea-879d-93b3b0ddbcec",
   "metadata": {},
   "source": [
    "## (2) Compare Speculative vs Autoregressive Sampling Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7121140-dd46-40b7-a5d3-8d548d24b383",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_speculative_sampling_vs_autoregressive_decoding(prompt: str,\n",
    "                                                            n_tokens_to_gen: int,\n",
    "                                                            temperature: float,\n",
    "                                                            K: int = 4):\n",
    "    print(f'Prompt: {prompt}')\n",
    "    indices = candle.Tensor([tokenizer.encode(prompt)])\n",
    "    \n",
    "    # Speculative sampling\n",
    "    \n",
    "    target_model.clear_kv_cache()\n",
    "    draft_model.clear_kv_cache()\n",
    "    generator = candle.nlp.speculative_sample(\n",
    "        target_model,\n",
    "        draft_model,\n",
    "        K=K,\n",
    "        indices=indices,\n",
    "        n_tokens_to_gen=n_tokens_to_gen,\n",
    "        top_k=40,\n",
    "        top_p=0.95,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    \n",
    "    start_time = time.time()\n",
    "    response = tokenizer.decode(np.concatenate(list(generator)))\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print('\\nSPECULATIVE SAMPLING')\n",
    "    print('====================')\n",
    "    print(f'Generation time: {end_time - start_time:.1f} sec')\n",
    "    print(f'Response: {response}')\n",
    "    \n",
    "    # Autoregressive decoding\n",
    "    \n",
    "    target_model.clear_kv_cache()\n",
    "    generator = candle.nlp.generation.batch_generation(\n",
    "        target_model, \n",
    "        indices=indices,\n",
    "        n_tokens_to_gen=n_tokens_to_gen,\n",
    "        top_k=40,\n",
    "        top_p=0.95,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    \n",
    "    start_time = time.time()\n",
    "    response = tokenizer.decode(np.concatenate(list(generator)))\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print('\\nAUTOREGRESSIVE DECODING')\n",
    "    print('====================')\n",
    "    print(f'Generation time: {end_time - start_time:.1f} sec')\n",
    "    print(f'Response: {response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957cf500-4397-400a-a7b2-74cfd7832246",
   "metadata": {},
   "source": [
    "> **Note:** we do not see benefits from speculative sampling in the below experiments. This is likely because the key observation from the paper, `the latency of parallel scoring of short continuations, generated by a faster but less powerful draft model, is comparable to that of sampling a single token from the larger target model` does not hold true in my single-threaded, throughput-bound, CPU-based setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5df7af71-0e16-44a3-b901-f95f4ba4a681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: The meaning of life is \n",
      "\n",
      "SPECULATIVE SAMPLING\n",
      "====================\n",
      "Generation time: 14.8 sec\n",
      "Response:  the meaning of life is the meaning of life is the meaning of life is the meaning of life is the meaning of life is the meaning of life is the meaning of life is the meaning of life is the meaning of life is the meaning of life\n",
      "\n",
      "AUTOREGRESSIVE DECODING\n",
      "====================\n",
      "Generation time: 13.1 sec\n",
      "Response:  the meaning of life is the meaning of life is the meaning of life is the meaning of life is the meaning of life is the meaning of life is the meaning of life is the meaning of life is the meaning of life is the meaning of life\n"
     ]
    }
   ],
   "source": [
    "compare_speculative_sampling_vs_autoregressive_decoding(\n",
    "    'The meaning of life is ',\n",
    "    n_tokens_to_gen=50,\n",
    "    temperature=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a50c5748-476d-4797-81f4-15b12f8c3b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Today on the Daily Show, \n",
      "\n",
      "SPECULATIVE SAMPLING\n",
      "====================\n",
      "Generation time: 20.5 sec\n",
      "Response:  Jon Stewart and Stephen Colbert took a look at the \"war on Christmas\" and how it's been used to justify the government's war on Christmas.\n",
      "The Daily Show: \"War on Christmas\"\n",
      "The Daily Show: \"War on Christmas\"\n",
      "\n",
      "AUTOREGRESSIVE DECODING\n",
      "====================\n",
      "Generation time: 13.0 sec\n",
      "Response:  Jon Stewart and Stephen Colbert took a look at the \"war on Christmas\" and how it's been used to justify the government's war on Christmas.\n",
      "The Daily Show: \"War on Christmas\"\n",
      "The Daily Show: \"War on Christmas\n"
     ]
    }
   ],
   "source": [
    "compare_speculative_sampling_vs_autoregressive_decoding(\n",
    "    'Today on the Daily Show, ',\n",
    "    n_tokens_to_gen=50,\n",
    "    temperature=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24785e7c-04c5-4e28-962e-f334f210e387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: GPT2 is the \n",
      "\n",
      "SPECULATIVE SAMPLING\n",
      "====================\n",
      "Generation time: 14.4 sec\n",
      "Response:  GPT2 of the GPT2 of the GPT2 of the GPT2 of the GPT2 of the GPT2 of the GPT2 of the GPT2 of the GPT2 of the GPT2 of\n",
      "\n",
      "AUTOREGRESSIVE DECODING\n",
      "====================\n",
      "Generation time: 13.2 sec\n",
      "Response:  GPT2 of the GPT2 of the GPT2 of the GPT2 of the GPT2 of the GPT2 of the GPT2 of the GPT2 of the GPT2 of the GPT2 of\n"
     ]
    }
   ],
   "source": [
    "compare_speculative_sampling_vs_autoregressive_decoding(\n",
    "    'GPT2 is the ',\n",
    "    n_tokens_to_gen=50,\n",
    "    temperature=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75b4eaff-6011-4adf-83c7-588c925f8bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Dolphins are \n",
      "\n",
      "SPECULATIVE SAMPLING\n",
      "====================\n",
      "Generation time: 25.9 sec\n",
      "Response: ileocarboxylic acids that contain approximately 50% to 85% propyl fatty acids and are derived from the seed oils of the citrus peels of the fruit.\n",
      "\n",
      "The most common phytochemicals found in coconut are laur\n",
      "\n",
      "AUTOREGRESSIVE DECODING\n",
      "====================\n",
      "Generation time: 13.0 sec\n",
      "Response: icky.\n",
      "\n",
      "The best part? They're not the worst thing to have on a car, unless you happen to live in Los Angeles.\n",
      "\n",
      "\"I've always been a fan of blue and white and I like the idea of a blue\n"
     ]
    }
   ],
   "source": [
    "compare_speculative_sampling_vs_autoregressive_decoding(\n",
    "    'Dolphins are ',\n",
    "    n_tokens_to_gen=50,\n",
    "    temperature=1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899693f4-13c5-4783-8426-583c16f794b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f761a330-cb33-4591-8847-379c9af9f468",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d71cbd-399d-45eb-b692-80c9ecdcba4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
